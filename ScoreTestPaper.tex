%% Paper implementing score test framework for SCR :)

\documentclass{article}
\usepackage{geometry}
\usepackage{layout}
%\geometry{papersize={297mm, 490mm}, left=15mm, right=15mm, top=5mm, bottom=5mm,
%  headheight=5mm, marginpar=5mm}
\usepackage{blindtext}
\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}
\usepackage{multicol}
%\usepackage{subfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage[parfill]{parskip} % Want line break between paragraphs instead of indentation
\usepackage{setspace} % For line spacing
\usepackage{lineno,xcolor} % For numbering lines

%% Bibliography, reference
\usepackage[backend=biber,citestyle=authoryear,uniquename=false,maxcitenames=2,
mincitenames=1,uniquelist=false, style=apa,giveninits=true]{biblatex} % For bibliography
\addbibresource{../../../../../PhDReferences_Zotero.bib} % Bibliography

%% Referencing stuff
\makeatletter
\newcommand{\apamaxcitenames}{2}

 \oddsidemargin  -10mm
 \evensidemargin -10mm
 \headheight -4mm
 \headsep -3mm
\textheight 250mm
\textwidth 180mm
\topmargin -4mm
\topskip -10mm

\title{Using score tests for model selection in spatial capture-recapture}

\begin{document}

\maketitle

% Setting double spacing
\doublespacing
% Adding line numbering
\linenumbers

\section{Introduction}
\label{sec:introduction}

Model selection is an extremely valuable and important step for any
statistical analysis, allowing the practitioner to confirm the
reliability of their chosen model and any resulting
conclusions. However, it is also a potentially fraught step of 
statistical analyses, especially when dealing with sophisticated
models that require substantial resources and time to fit. In such
instances, fitting every candidate model and carrying 
out model selection using likelihood ratio tests (LRTs) or comparing
AIC values, for example, can be impractical and unrealistic. An
absence of viable alternatives when working with elaborate models
often means these traditional model selection 
methods are the only option, compromising the practitioner’s
ability to confidently select an appropriate model for their analysis.

To address this issue, \textcite{catchpoleMorgan1996} propose a
framework harnessing score tests for model selection, greatly reducing
the number of candidate models that need to be fitted 
to select an appropriate model for the data. To date, it has
been applied extensively to the field of capture-recapture, e.g. \textcite{mccreaMorgan2011},
\textcite{mccreaMorganBreg2012} and \textcite{mccreaMorganGim2017}. 

To implement the score test framework, the candidate set is organised
into levels and, beginning with the simplest models, score tests
define a path through these levels to select the
final model \parencite{mccreaMorgan2011}. We begin by identifying the 
most basic model -- this model 
makes up Level 0 of the candidate set. Level 1 contains all models
with one added parameter dependency. We begin with the null hypothesis
that the Level 0 model is adequate for the data. Therefore, we refer to
the Level 0 model as the `$H_0$ model'. Our
alternative hypotheses are that each Level 1 model provides a
significantly better fit, and we refer to these models as `$H_1$ models'. We conduct
score tests comparing the null hypothesis to each alternative
hypothesis, i.e. comparing the $H_0$ model to each $H_1$ model. The $H_1$ model for the 
test with the smallest significant 
p-value defines the new null hypothesis -- that is, it becomes the new
$H_0$ model. Level 2 is then constructed,
consisting of all models with one additional parameter dependency
relative to this model, and the process is repeated. We stop
when none of the score tests produce significant p-values. At this
point, the $H_0$ model is the final chosen model.

When conducting a score test comparing two nested models, only the simpler
of the two models needs to be fitted \textbf{(cite)}. So at
each step of the process, only the $H_0$ model needs to be
fitted. Furthermore, the stepwise nature of the 
framework enhances the convenience of any model-fitting -- the MLEs
from a given $H_0$ model may provide 
appropriate starting values for fitting the next $H_0$ model
\parencite{mccreaMorgan2011}. The issue of choosing a model from a 
potentially vast candidate set of complicated models is reduced to the
task of calculating score statistics and fitting a few, potentially
relatively simple, models.

However, despite these benefits, the application of the framework has
not extended beyond capture-recapture models. The reason is likely
two-fold. Firstly, depending on the likelihood equations for the models
being considered, it may appear prohibitively inconvenient to
obtain the required score vectors and information matrices via
exact calculations or numerical approximations. Previous infrastructure
likely made model-fitting, and therefore traditional model selection
techniques, more attractive in comparison. Secondly, depending on the
nature of the models being appraised, there may be a lack of a clear
nesting structure in the candidate set appropriate for score tests,
making it unclear how to organise models into levels as described above.

For example, although a natural extension of the previous applications to
capture-recapture, the score test framework has not yet been applied to
spatial capture-recapture (SCR) models. Likelihood equations for these
models can be quite sophisticated -- any gradients cannot be found
symbolically, statistical software must be employed. Additionally, not all SCR
models can be organised into a clear nested structure that is
suitable for score tests. Depending on
the parameter dependencies present in the candidate set, it may not be possible
to build the model levels as described above. Such issues are
common and pose significant obstacles to the widespread use of this
seemingly convenient and helpful tool for model selection.

In this paper, we address these issues. We propose the use of the
 `RTMB' R package to conveniently obtain exact gradients for all models
being considered when implementing the score test framework. We also
propose an extension to the framework, allowing for its use when the
candidate set lacks an appropriate nested structure. We present these
novel findings via an illustrative example -- we will implement the
score test framework using the ovenbird spatial capture-recapture data
set, available in the `secr' package.

As with the potential difficulties of model selection, our insights
are not specific to any one field of research. They may be 
applied to a vast range of models. Our findings aid in
enhancing the general utility of the score test framework, thereby
enhancing the accessibility of model selection, in general.

\section{Illustrative data set: ovenbirds}
\label{sec:ovenbirds}

In \textcite{borchersEfford2008}, alongside introducing SCR models,
the authors illustrate a potential approach 
for SCR model selection. The data set consists of captures of birds
(red-eyed vireos) from a mist-netting survey on the Patuxent Research Refuge 
in Maryland, USA, conducted by C.S. Robbins from 1961 to 1972. For
model selection, a variety of SCR
models are fitted and the final model is selected via AIC.

Since this research was published, there has been some exploration
into alternative model selection methods for SCR. However, model
selection via AIC remains an accessible and common approach.

For easy comparison to the standard set by such a well-known example, we
work with a similar data set. The `ovenbird' data
set, available via the `secr' R package, contains SCR data from a
mist-netting survey of ovenbirds conducted from 2005 to 2009 at the Patuxent
Research Refuge in Maryland, USA by D. K. Dawson and M. G. Efford. As
described in the package documentation, the forest in the area has
changed little since the previous survey conducted by C.S. Robbins
\textbf{(cite)}.

Forty four mist nets were passively operated for 9 or
10 non-consecutive days in late May and June of each year. The data
set consists of adult ovenbird captures, with one bird
 killed by a predator while in a net in 2009 \textbf{(cite)}. For
our analysis, we removed this loss from the data set. We retain any
records of individual birds being captured more than once in a day. As
is standard when model fitting, we treat each annual sample as if
from an independent closed population \textbf{(cite -- Borchers
  and Efford 2008 or the book.)}

Our candidate model set contains all SCR models that share parameter
dependencies considered in
\textcite{borchersEfford2008}. The latter candidate set contains
fifteen models. By considering all possible combinations of the
parameter dependencies presented in \textcite{borchersEfford2008}, our
set contains forty five models. See \textbf{Table (results)} for a
description of the models considered. 

To construct an accurate picture of the benefits afforded by
the score test framework, alongside implementing this framework, we
fit all models in our candidate set using the `secr' R package and
select the final model via AIC. We compare a new, more efficient,
approach to model selection with a widely-used traditional approach,
in the context of an example that would be familiar to practitioners.

\section{Implementing the score test framework}
\label{sec:implementing}

Working with forty five candidate SCR models, we extend the score test
framework presented by \textcite{catchpoleMorgan1996}, applying it
to a comprehensive candidate set in which all models do not fit neatly
into a nested structure appropriate for score tests.

\subsection{Calculating score test statistics with RTMB}
\label{sec:calculating-stats-rtmb}

{Explanation of how to calculate score test statistics. Include
  intuitive explanations of what gradients tell us, how they
  inflate/deflate statistics, what we are looking for overall
  (distance between H0 MLE vec and H1 peak). Then go into
  benefits of RTMB.}

\subsection{Issues with nesting}
\label{sec:nesting-issues}

% For two models to be nested appropriately for score tests, it must be
% possible to obtain the simpler model from the more complex one by
% placing fixed constraints on any additional parameters. Any small changes in
% the values of these constrained parameters must change the
% likelihood. This ensures the $H_1$ log-likelihood has curvature at the
% $H_0$ estimates, allowing us to calculate the required gradients.

% From ChatGPT (for own understanding):
% For a score test, we only need the likelihood to have curvature in the
% directions of the parameters being tested under H_0 (psi), because the
% test measures the sensitivity of the likelihood to changes in those
% parameters. Curvature in other parameters is not required — it does
% not affect the calculation of the score statistic or its chi-square
% distribution.
% The score test looks only at the change in likelihood if we move the
% additional parameters psi away from their null values (constrained values under
% H_0).

\textbf{(Perhaps refine with lambda and psi notation, if used in
  section above?)}

For two nested models to be appropriate for a score test, it must be
possible to obtain the simpler model (the $H_0$ model) from the more
complex one (the $H_1$ model) via fixed constraints on any additional
parameters, $\boldsymbol{\psi}$. All parameters in
$\boldsymbol{\psi}$ must be identifiable 
in the context of the $H_0$ model -- i.e. when identifying the point on the
$H_1$ likelihood surface equivalent to the $H_0$ MLE vector,
there must be only one choice. When this holds, any small deviations
from this point cause a
change in the likelihood, ensuring the $H_1$ log-likelihood 
at the $H_0$ MLE vector has the required curvature in the direction of
all parameters in $\boldsymbol{\psi}$. Without this curvature, it is no longer clear how
far the $H_0$ MLE vector is from the peak of the $H_1$ log-likelihood
-- that is, it is unclear how much the additional parameters
in $\boldsymbol{\psi}$ improve fit. Any 
score statistics are degenerate and the 
asymptotic Chi-squared distribution gives unusable
p-values. 

In our example, issues arise as we consider models that allow for individual
heterogeneity. As shown in \textbf{Table (results)}, our candidate set
contains models that have a two-class finite mixture dependency for $g_0$ and/or
$\sigma$. These models allow individuals in the population to belong to
one of two classes, where each class is defined by its corresponding estimate of $g_0$
and/or $\sigma$ \textbf{(cite -- SECR book)}. The probability of
belonging to either class is estimated by the model. We refer to this
probability estimate as $\hat{p}$.

% Due to their nature, when conducting score tests, models with mixture
% models can only be compared to models with matching mixture
% components.  
% Consider the example where
% our $H_0$ model has no mixture component 
% and our $H_1$ model has a mixture component for $g_0$. 


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
